{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled21.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPlf/n2XqBHn3U7+AFsh6Ae",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/thazin31086/GitHub_NeturalNetworks/blob/master/IssueReport%20Similarity%20Grouping.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6TjhJuc9TAN4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 63
        },
        "outputId": "ad101cbc-51b9-427e-9042-c248277571cd"
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import nltk\n",
        "import re\n",
        "import nltk\n",
        "#nltk.download('punkt')\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from string import punctuation\n",
        "\n",
        "# tensroflow hub module for Universal sentence Encoder \n",
        "module_url = \"https://tfhub.dev/google/universal-sentence-encoder-large/3\" #@param [\"https://tfhub.dev/google/universal-sentence-encoder/2\", \"https://tfhub.dev/google/universal-sentence-encoder-large/3\"]\n",
        "embed = hub.Module(module_url)\n",
        "\n",
        "def get_features(texts):\n",
        "    if type(texts) is str:\n",
        "        texts = [texts]\n",
        "    with tf.Session() as sess:\n",
        "        sess.run([tf.global_variables_initializer(), tf.tables_initializer()])\n",
        "        return sess.run(embed(texts))\n",
        "\n",
        "def remove_stopwords(stop_words, tokens):\n",
        "    res = []\n",
        "    for token in tokens:\n",
        "        if not token in stop_words:\n",
        "            res.append(token)\n",
        "    return res\n",
        "\n",
        "def process_text(text):\n",
        "    text = text.encode('ascii', errors='ignore').decode()\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'http\\S+', ' ', text)\n",
        "    text = re.sub(r'#+', ' ', text )\n",
        "    text = re.sub(r'@[A-Za-z0-9]+', ' ', text)\n",
        "    text = re.sub(r\"([A-Za-z]+)'s\", r\"\\1 is\", text)\n",
        "    #text = re.sub(r\"\\'s\", \" \", text)     text = re.sub(r\"\\'ve\", \" have \", text)\n",
        "    text = re.sub(r\"won't\", \"will not \", text)\n",
        "    text = re.sub(r\"isn't\", \"is not \", text)\n",
        "    text = re.sub(r\"can't\", \"can not \", text)\n",
        "    text = re.sub(r\"n't\", \" not \", text)\n",
        "    text = re.sub(r\"i'm\", \"i am \", text)\n",
        "    text = re.sub(r\"\\'re\", \" are \", text)\n",
        "    text = re.sub(r\"\\'d\", \" would \", text)\n",
        "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
        "    text = re.sub('\\W', ' ', text)\n",
        "    text = re.sub(r'\\d+', ' ', text)\n",
        "    text = re.sub('\\s+', ' ', text)\n",
        "    text = text.strip()\n",
        "    return text\n",
        "\n",
        "\n",
        "def lemmatize(tokens):\n",
        "    lemmatizer = nltk.stem.WordNetLemmatizer()\n",
        "    lemma_list = []\n",
        "    for token in tokens:\n",
        "        lemma = lemmatizer.lemmatize(token, 'v')\n",
        "        if lemma == token:\n",
        "            lemma = lemmatizer.lemmatize(token)\n",
        "        lemma_list.append(lemma)\n",
        "    # return [ lemmatizer.lemmatize(token, 'v') for token in tokens ]     return lemma_list\n",
        "\n",
        "\n",
        "def process_all(text):\n",
        "    text = process_text(text)\n",
        "    return ' '.join(remove_stopwords(stop_words, text.split()))\n",
        "\n",
        "def cosine_similarity(v1, v2):\n",
        "    mag1 = np.linalg.norm(v1)\n",
        "    mag2 = np.linalg.norm(v2)\n",
        "    if (not mag1) or (not mag2):\n",
        "        return 0\n",
        "    return np.dot(v1, v2) / (mag1 * mag2)\n",
        "\n",
        "def test_similarity(text1, text2):\n",
        "    vec1 = get_features(text1)[0]\n",
        "    vec2 = get_features(text2)[0]\n",
        "    print(vec1.shape)\n",
        "    return cosine_similarity(vec1, vec2)\n",
        "\n",
        "def semantic_search(query, data, vectors):\n",
        "    query = process_text(query)\n",
        "    print(\"Extracting features...\")\n",
        "    query_vec = get_features(query)[0].ravel()\n",
        "    res = []\n",
        "    for i, d in enumerate(data):\n",
        "        qvec = vectors[i].ravel()\n",
        "        sim = cosine_similarity(query_vec, qvec)\n",
        "        res.append((sim, d[:100], i))\n",
        "    return sorted(res, key=lambda x : x[0], reverse=True)\n",
        "\n",
        "def unique_words(sentence):\n",
        "    return set(sentence.lower().split())\n",
        " "
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fkOqVRTOVF-u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = [\n",
        "    \"Avoid crash on concat on structs with ToString member\", #https://github.com/dotnet/roslyn/pull/38860/commits\n",
        "    \"Enum implicit cast to string fails when element is named ToString\", #https://github.com/dotnet/roslyn/issues/40256\n",
        "    \"Enum with ToString member crashes in string concatenation\", #https://github.com/dotnet/roslyn/issues/38858\n",
        "    \"Visual Studio 2019 crashing when click RMB on rule in Analyzers' dependencies\",#https://github.com/dotnet/roslyn/issues/40720\n",
        "    \"Crash on right-click a analyse rule in Solution-Explorer\", #https://github.com/dotnet/roslyn/issues/36304\n",
        "    \"Handle lazy loading of analyzer command handlers\", #https://github.com/dotnet/roslyn/pull/36740\n",
        "]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6M45sIQqVQ7j",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "outputId": "18e44962-77e8-49d0-b13f-3af67ee711d2"
      },
      "source": [
        "data_processed = list(map(process_text, data))\n",
        "BASE_VECTORS = get_features(data)\n",
        "#semantic_search(\"Avoid crash on concat on structs with ToString member\", data_processed, BASE_VECTORS)\n",
        "semantic_search(\"Visual Studio 2019 crashing when click RMB on rule in Analyzers' dependencies\", data_processed, BASE_VECTORS)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting features...\n",
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(0.98528624,\n",
              "  'visual studio crashing when click rmb on rule in analyzers dependencies',\n",
              "  3),\n",
              " (0.699809, 'avoid crash on concat on structs with tostring member', 0),\n",
              " (0.67198277, 'enum with tostring member crashes in string concatenation', 2),\n",
              " (0.64385056, 'crash on right click a analyse rule in solution explorer', 4),\n",
              " (0.63877857,\n",
              "  'enum implicit cast to string fails when element is named tostring',\n",
              "  1),\n",
              " (0.6170363, 'handle lazy loading of analyzer command handlers', 5)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KOnVrJsZXgNP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 230
        },
        "outputId": "5c5bf2a9-16f2-4d54-e584-d2f1589287a8"
      },
      "source": [
        "#Compare with Code\n",
        "issuereport_uniquewordlist = unique_words(' '.join(data[3:6]))\n",
        "issuereport_cluser1 = ' '.join(issuereport_uniquewordlist)\n",
        "codedata = ['project should other severity link get changed add item name items with hierarchy create checked show help initialize rule active context handler new folder enabled any controller menu diagnostics copy file update diagnostic analyzer open analyzers selected command remove set', \n",
        "            'Analyzer Command Handler Tests Traits Features Diagnostics Diagnostic Context Menu Controller Solution Explorer']\n",
        "codedata_processed = list(map(process_text, codedata))          \n",
        "code_BASE_VECTORS = get_features(codedata)\n",
        "\n",
        "print(issuereport_cluser1)\n",
        "semantic_search(issuereport_cluser1, codedata_processed, code_BASE_VECTORS)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "right-click crashing when crash in 2019 of loading click studio a on command analyzer dependencies visual solution-explorer rmb analyzers' handle lazy handlers rule analyse\n",
            "Extracting features...\n",
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(0.7856611,\n",
              "  'analyzer command handler tests traits features diagnostics diagnostic context menu controller soluti',\n",
              "  1),\n",
              " (0.64409107,\n",
              "  'project should other severity link get changed add item name items with hierarchy create checked sho',\n",
              "  0)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    }
  ]
}