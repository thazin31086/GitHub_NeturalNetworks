{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PredictDeveloper.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "bJdVqM3c50Pv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#pip install --upgrade tensorflow-gpu\n",
        "import tensorflow as tf\n",
        "import time\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import keras\n",
        "import numpy as np\n",
        "import tensorflow_datasets as tfds\n",
        "import tensorflow.keras.backend as K\n",
        "from tensorflow.keras.layers import Input, Lambda, Dense\n",
        "from tensorflow.keras.models import Model\n",
        "from sklearn import preprocessing\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.layers import Dense, Flatten, Dropout, Conv1D, Reshape, Concatenate"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BZpf_XvM7iez",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = pd.read_csv('QDeveloper.csv')\n",
        "data = data.sample(frac=1)\n",
        "data = data.fillna('unknown')\n",
        "y = list(data['FixedByID']) \n",
        "x_context = list(data['Title_Description'])\n",
        "data.AST = data.AST.astype(str)\n",
        "x_AST = list(data['AST'])\n",
        "\n",
        "le = preprocessing.LabelEncoder()\n",
        "le.fit(y)\n",
        "\n",
        "categories = list(set(y))\n",
        "\n",
        "def encode(le, labels):\n",
        "    enc = le.transform(labels)\n",
        "    #return keras.utils.to_categorical(enc)\n",
        "    return enc\n",
        "\n",
        "def decode(le, labels):\n",
        "    #dec = np.argmax(one_hot, axis=1)\n",
        "    #dec = np.argmax(one_hot)\n",
        "    dec = le.inverse_transform(labels)\n",
        "    return dec\n",
        "\n",
        "test = encode(le,categories)\n",
        "untest = decode(le, test)\n",
        "\n",
        "\n",
        "y_enc = encode(le, y)\n",
        "\n",
        "#80% / 20% train / test split:\n",
        "\n",
        "train_size = int(len(x_context) * .8)\n",
        "\n",
        "#x_train = np.asarray(x_enc[:train_size])\n",
        "#y_train = np.asarray(y_enc[:train_size])\n",
        "\n",
        "#x_test = np.asarray(x_enc[train_size:])\n",
        "#y_test = np.asarray(y_enc[train_size:]) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cikW9zRrXMuL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "x_train_context = x_context[:train_size]\n",
        "x_train_AST = x_AST[:train_size]\n",
        "y_train = y_enc[:train_size]\n",
        "\n",
        "x_test_context = x_context[train_size:]\n",
        "x_test_AST = x_AST[train_size:]\n",
        "y_test = y_enc[train_size:]\n",
        "\n",
        "# convert string to lower case \n",
        "x_train_context = [s.lower() for s in x_train_context]\n",
        "x_test_context = [s.lower() for s in x_test_context]\n",
        "#=======================Convert string to index================\n",
        "# Tokenizer\n",
        "tk_context = Tokenizer(num_words=None, char_level=None, oov_token='Unknown')\n",
        "tk_context.fit_on_texts(x_context)\n",
        "\n",
        "tk_AST = Tokenizer(num_words=None, char_level=None, oov_token='Unknown')\n",
        "tk_AST.fit_on_texts(x_AST)\n",
        "\n",
        "# Convert string to index \n",
        "x_train_context_sequences = tk_context.texts_to_sequences(x_train_context)\n",
        "x_train_AST_sequences = tk_context.texts_to_sequences(x_train_AST)\n",
        "x_test_context_sequences = tk_AST.texts_to_sequences(x_test_context)\n",
        "x_test_AST_sequences = tk_AST.texts_to_sequences(x_test_AST)\n",
        "\n",
        "# Padding\n",
        "x_train_context = pad_sequences(x_train_context_sequences, maxlen=1000, padding='post')\n",
        "x_train_AST = pad_sequences(x_train_AST_sequences, maxlen=1000, padding='post')\n",
        "x_test_context = pad_sequences(x_test_context_sequences, maxlen=1000, padding='post')\n",
        "x_test_AST = pad_sequences(x_test_AST_sequences, maxlen=1000, padding='post')\n",
        "\n",
        "# Convert to numpy array\n",
        "x_train_context = np.array(x_train_context)\n",
        "x_train_AST = np.array(x_train_AST)\n",
        "x_test_context = np.array(x_test_context)\n",
        "x_test_AST = np.array(x_test_AST)\n",
        "\n",
        "#y_train_format = tf.shape(tf.squeeze(y_train[0]))\n",
        "#y_test_format = tf.shape(tf.squeeze(y_test[0]))\n",
        "\n",
        "trainingcontextdataset = (\n",
        "    tf.data.Dataset.from_tensor_slices(\n",
        "        (\n",
        "             tf.cast(x_train_context, tf.float32),\n",
        "             tf.cast(y_train, tf.float32)          \n",
        "        )\n",
        "    )\n",
        ")\n",
        "\n",
        "trainingASTdataset = (\n",
        "    tf.data.Dataset.from_tensor_slices(\n",
        "        (\n",
        "             tf.cast(x_train_AST, tf.float32),\n",
        "             tf.cast(y_train, tf.float32)          \n",
        "        )\n",
        "    )\n",
        ")\n",
        "\n",
        "testcontextdataset = (\n",
        "    tf.data.Dataset.from_tensor_slices(\n",
        "        (\n",
        "            tf.cast(x_test_context, tf.float16),\n",
        "            tf.cast(y_test, tf.float32)           \n",
        "        )\n",
        "    )\n",
        ")\n",
        "\n",
        "testASTdataset = (\n",
        "    tf.data.Dataset.from_tensor_slices(\n",
        "        (\n",
        "            tf.cast(x_test_AST, tf.float16),\n",
        "            tf.cast(y_test, tf.float32)           \n",
        "        )\n",
        "    )\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AU_50G6-8yhJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_angles(pos, i, d_model):\n",
        "  angle_rates = 1 / np.power(10000, (2 * (i//2)) / np.float32(d_model))\n",
        "  return pos * angle_rates\n",
        "  \n",
        "def positional_encoding(position, d_model):\n",
        "  angle_rads = get_angles(np.arange(position)[:, np.newaxis],\n",
        "                          np.arange(d_model)[np.newaxis, :],\n",
        "                          d_model)\n",
        "  \n",
        "  # apply sin to even indices in the array; 2i\n",
        "  angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
        "  \n",
        "  # apply cos to odd indices in the array; 2i+1\n",
        "  angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
        "    \n",
        "  pos_encoding = angle_rads[np.newaxis, ...]\n",
        "    \n",
        "  return tf.cast(pos_encoding, dtype=tf.float32)\n",
        "\n",
        "def point_wise_feed_forward_network(d_model, dff):\n",
        "  return tf.keras.Sequential([\n",
        "      tf.keras.layers.Dense(dff, activation='relu'),  # (batch_size, seq_len, dff)\n",
        "      tf.keras.layers.Dense(d_model)  # (batch_size, seq_len, d_model)\n",
        "  ])\n",
        "\n",
        "def scaled_dot_product_attention(q, k, v, mask):\n",
        "  \"\"\"Calculate the attention weights.\n",
        "  q, k, v must have matching leading dimensions.\n",
        "  k, v must have matching penultimate dimension, i.e.: seq_len_k = seq_len_v.\n",
        "  The mask has different shapes depending on its type(padding or look ahead) \n",
        "  but it must be broadcastable for addition.\n",
        "  \n",
        "  Args:\n",
        "    q: query shape == (..., seq_len_q, depth)\n",
        "    k: key shape == (..., seq_len_k, depth)\n",
        "    v: value shape == (..., seq_len_v, depth_v)\n",
        "    mask: Float tensor with shape broadcastable \n",
        "          to (..., seq_len_q, seq_len_k). Defaults to None.\n",
        "    \n",
        "  Returns:\n",
        "    output, attention_weights\n",
        "  \"\"\"\n",
        "\n",
        "  matmul_qk = tf.matmul(q, k, transpose_b=True)  # (..., seq_len_q, seq_len_k)\n",
        "  \n",
        "  # scale matmul_qk\n",
        "  dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
        "  scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
        "\n",
        "  # add the mask to the scaled tensor.\n",
        "  if mask is not None:\n",
        "    scaled_attention_logits += (mask * -1e9)  \n",
        "\n",
        "  # softmax is normalized on the last axis (seq_len_k) so that the scores\n",
        "  # add up to 1.\n",
        "  attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)  # (..., seq_len_q, seq_len_k)\n",
        "\n",
        "  output = tf.matmul(attention_weights, v)  # (..., seq_len_q, depth_v)\n",
        "\n",
        "  return output, attention_weights\n",
        "\n",
        "def create_padding_mask(seq):\n",
        "  seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
        "  \n",
        "  # add extra dimensions to add the padding\n",
        "  # to the attention logits.\n",
        "  return seq[:, tf.newaxis, tf.newaxis, :]  # (batch_size, 1, 1, seq_len)\n",
        "  \n",
        "def create_masks(inp):\n",
        "    # Encoder padding mask\n",
        "    enc_padding_mask = create_padding_mask(inp)\n",
        "\n",
        "    return enc_padding_mask\n",
        "\n",
        "def TransformerEncoder(x, contextflag):   \n",
        "   if contextflag == True:\n",
        "     ##################Initalize Context Variable###################\n",
        "     input_vocab_size = len(tk_context.word_index) + 2\n",
        "     target_vocab_size = len(tk_context.word_index) + 2\n",
        "     enc_padding_mask = create_masks(x)\n",
        "     tranformer = Transformer(num_layers, d_model, num_heads, dff,\n",
        "                          len(tk_context.word_index) + 1, len(tk_context.word_index) + 1, dropout_rate)\n",
        "     t_out = tranformer(x, True,enc_padding_mask)\n",
        "   else:\n",
        "     input_vocab_size = len(tk_AST.word_index) + 2\n",
        "     target_vocab_size = len(tk_AST.word_index) + 2\n",
        "     enc_padding_mask = create_masks(x)\n",
        "     tranformer = Transformer(num_layers, d_model, num_heads, dff,\n",
        "                          len(tk_context.word_index) + 1, len(tk_context.word_index) + 1, dropout_rate)\n",
        "     t_out = tranformer(x, True,enc_padding_mask)\n",
        "\n",
        "   return t_out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dCMQrGXaF6ef",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MultiHeadAttention(tf.keras.layers.Layer):\n",
        "  def __init__(self, d_model, num_heads):\n",
        "    super(MultiHeadAttention, self).__init__()\n",
        "    self.num_heads = num_heads\n",
        "    self.d_model = d_model\n",
        "    \n",
        "    assert d_model % self.num_heads == 0\n",
        "    \n",
        "    self.depth = d_model // self.num_heads\n",
        "    \n",
        "    self.wq = tf.keras.layers.Dense(d_model)\n",
        "    self.wk = tf.keras.layers.Dense(d_model)\n",
        "    self.wv = tf.keras.layers.Dense(d_model)\n",
        "    \n",
        "    self.dense = tf.keras.layers.Dense(d_model)\n",
        "        \n",
        "  def split_heads(self, x, batch_size):\n",
        "    \"\"\"Split the last dimension into (num_heads, depth).\n",
        "    Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
        "    \"\"\"\n",
        "    x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
        "    return tf.transpose(x, perm=[0, 2, 1, 3])\n",
        "    \n",
        "  def call(self, v, k, q, mask):\n",
        "    batch_size = tf.shape(q)[0]\n",
        "    \n",
        "    q = self.wq(q)  # (batch_size, seq_len, d_model)\n",
        "    k = self.wk(k)  # (batch_size, seq_len, d_model)\n",
        "    v = self.wv(v)  # (batch_size, seq_len, d_model)\n",
        "    \n",
        "    q = self.split_heads(q, batch_size)  # (batch_size, num_heads, seq_len_q, depth)\n",
        "    k = self.split_heads(k, batch_size)  # (batch_size, num_heads, seq_len_k, depth)\n",
        "    v = self.split_heads(v, batch_size)  # (batch_size, num_heads, seq_len_v, depth)\n",
        "    \n",
        "    # scaled_attention.shape == (batch_size, num_heads, seq_len_q, depth)\n",
        "    # attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)\n",
        "    scaled_attention, attention_weights = scaled_dot_product_attention(\n",
        "        q, k, v, mask)\n",
        "    \n",
        "    scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])  # (batch_size, seq_len_q, num_heads, depth)\n",
        "\n",
        "    concat_attention = tf.reshape(scaled_attention, \n",
        "                                  (batch_size, -1, self.d_model))  # (batch_size, seq_len_q, d_model)\n",
        "\n",
        "    output = self.dense(concat_attention)  # (batch_size, seq_len_q, d_model)\n",
        "        \n",
        "    return output, attention_weights\n",
        "\n",
        "class EncoderLayer(tf.keras.layers.Layer):\n",
        "  def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
        "    super(EncoderLayer, self).__init__()\n",
        "\n",
        "    self.mha = MultiHeadAttention(d_model, num_heads)\n",
        "    self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
        "\n",
        "    self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "    self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "    \n",
        "    self.dropout1 = tf.keras.layers.Dropout(rate)\n",
        "    self.dropout2 = tf.keras.layers.Dropout(rate)\n",
        "    \n",
        "  def call(self, x, training, mask):\n",
        "\n",
        "    attn_output, _ = self.mha(x, x, x, mask)  # (batch_size, input_seq_len, d_model)\n",
        "    attn_output = self.dropout1(attn_output, training=training)\n",
        "    out1 = self.layernorm1(x + attn_output)  # (batch_size, input_seq_len, d_model)\n",
        "    \n",
        "    ffn_output = self.ffn(out1)  # (batch_size, input_seq_len, d_model)\n",
        "    ffn_output = self.dropout2(ffn_output, training=training)\n",
        "    out2 = self.layernorm2(out1 + ffn_output)  # (batch_size, input_seq_len, d_model)    \n",
        "    return out2\n",
        "\n",
        "class Encoder(tf.keras.layers.Layer):\n",
        "  def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size,\n",
        "               maximum_position_encoding, rate=0.1):\n",
        "    super(Encoder, self).__init__()\n",
        "\n",
        "    self.d_model = d_model\n",
        "    self.num_layers = num_layers\n",
        "    \n",
        "    self.embedding = tf.keras.layers.Embedding(input_vocab_size, d_model)\n",
        "    self.pos_encoding = positional_encoding(maximum_position_encoding, \n",
        "                                            self.d_model)   \n",
        "    \n",
        "    self.enc_layers = [EncoderLayer(d_model, num_heads, dff, rate) \n",
        "                       for _ in range(num_layers)]\n",
        "  \n",
        "    self.dropout = tf.keras.layers.Dropout(rate)\n",
        "        \n",
        "  def call(self, x, training, mask):\n",
        "\n",
        "    seq_len = tf.shape(x)[1]\n",
        "\n",
        "    # adding embedding and position encoding.\n",
        "    x = self.embedding(x)  # (batch_size, input_seq_len, d_model)\n",
        "    x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "    x += self.pos_encoding[:, :seq_len, :]\n",
        "\n",
        "    x = self.dropout(x, training=training)\n",
        "    \n",
        "    for i in range(self.num_layers):\n",
        "      x = self.enc_layers[i](x, training, mask)\n",
        "    \n",
        "    return x  # (batch_size, input_seq_len, d_model)\n",
        "\n",
        "class Transformer(tf.keras.Model):\n",
        "    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, \n",
        "                   target_vocab_size, rate=0.1):\n",
        "        super(Transformer, self).__init__()\n",
        "\n",
        "        self.encoder = Encoder(num_layers, d_model, num_heads, dff, \n",
        "                               input_vocab_size, rate)\n",
        "\n",
        "        self.dense1 = tf.keras.layers.Dense(d_model, activation='tanh')\n",
        "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
        "        self.dense2 = tf.keras.layers.Dense(256, activation='tanh')\n",
        "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
        "        self.final_layer = tf.keras.layers.Dense(len(categories), activation='softmax')\n",
        "        #self.final_layer = tf.keras.layers.Dense(128, activation='relu')\n",
        "\n",
        "    def call(self, inp, training, enc_padding_mask):\n",
        "        enc_output = self.encoder(inp, training, enc_padding_mask)\n",
        "        enc_output = self.dense1(enc_output[:,0])\n",
        "        enc_output = self.dropout1(enc_output, training=training)\n",
        "        enc_output = self.dense2(enc_output)\n",
        "        enc_output = self.dropout2(enc_output, training=training)\n",
        "        final_output = self.final_layer(enc_output)\n",
        "        return final_output\n",
        "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
        "    def __init__(self, d_model, warmup_steps=4000):\n",
        "        super(CustomSchedule, self).__init__()\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.d_model = tf.cast(self.d_model, tf.float32)\n",
        "\n",
        "        self.warmup_steps = warmup_steps\n",
        "\n",
        "    def __call__(self, step):\n",
        "        arg1 = tf.math.rsqrt(step)\n",
        "        arg2 = step * (self.warmup_steps ** -1.5)\n",
        "\n",
        "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SO16TAmGTYf2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "###########################Initalize Variable#####################################\n",
        "num_layers = 6\n",
        "d_model = 512\n",
        "dff = 2048\n",
        "num_heads = 8\n",
        "\n",
        "dropout_rate = 0.1\n",
        "learning_rate = CustomSchedule(d_model)\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lMAQeEEshUd8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "TransformerEncoder(x_train_context[22:23],True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-9zVvm3DhdGk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "TransformerEncoder(x_train_AST[5:6], False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tCcmxNb9hymU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.keras.layers.Concatenate()([TransformerEncoder(x_train_context[22:23],True), TransformerEncoder(x_train_AST[5:6], False)]).shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "upL1LqAjGWFU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_layers = 6\n",
        "d_model = 512\n",
        "dff = 2048\n",
        "num_heads = 8\n",
        "dropout_rate = 0.1\n",
        "enc_padding_mask = create_masks(x_train_AST[5:6])\n",
        "input_vocab_size = len(tk_AST.word_index) + 2\n",
        "target_vocab_size = len(tk_AST.word_index) + 2\n",
        "embedding = tf.keras.layers.Embedding(input_vocab_size, d_model)\n",
        "x = embedding(enc_padding_mask)  # (batch_size, input_seq_len, d_model)\n",
        "x *= tf.math.sqrt(tf.cast(d_model, tf.float32))\n",
        "x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jDsDHIXRiYPN",
        "colab_type": "code",
        "outputId": "1131ee39-7b3a-4b72-f3cb-3fd25828cab6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "def codeembedding(x): \n",
        "    embedsize = len(tk_context.word_index) + 1\n",
        "    result=  tf.keras.layers.Embedding(embedsize, output_dim=d_model)(x)\n",
        "    return result\n",
        "\n",
        "\n",
        "input_context = Input(shape=(1000,), dtype=tf.float32)\n",
        "input_AST = Input(shape=(1000,), dtype=tf.float32)\n",
        "\n",
        "contextmodel = Model(inputs=input_context, outputs=TransformerEncoder(input_context,True))\n",
        "#codemodel = Model(inputs=input_AST, outputs=TransformerEncoder(input_AST, False))\n",
        "\n",
        "codedense = tf.keras.layers.Dense(256, activation='relu')(codeembedding(input_AST))\n",
        "LSTM = tf.keras.layers.LSTM(128)(codedense)\n",
        "outputs = tf.keras.layers.Dense(len(categories), activation='softmax')(LSTM)\n",
        "codemodel = Model(inputs=input_AST, outputs=outputs)\n",
        "\n",
        "# the Context and AST\n",
        "combinedInput = tf.keras.layers.Concatenate()([contextmodel.output, codemodel.output])\n",
        "\n",
        "#final_Output Softmax\n",
        "#final_output = Dense(536, activation=\"relu\")(codemodel)\n",
        "final_output = Dense(258, activation=\"tanh\")(combinedInput)\n",
        "final_output = Dense(len(categories), activation=\"softmax\")(combinedInput)\n",
        "\n",
        "model = Model(inputs=[contextmodel.input, codemodel.input], outputs=final_output)\n",
        "\n",
        "model.compile(optimizer=optimizer,\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "model.fit([x_train_context, x_train_AST], y_train, epochs=5)\n",
        "model.evaluate([x_test_context, x_test_AST],y_test)\n",
        "model.save_weights('./model_PreditDeveloper.h5')"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "42/42 [==============================] - 2241s 53s/step - loss: 5.5898 - accuracy: 0.0061\n",
            "Epoch 2/5\n",
            "42/42 [==============================] - 2221s 53s/step - loss: 5.5847 - accuracy: 0.0061\n",
            "Epoch 3/5\n",
            "42/42 [==============================] - 2228s 53s/step - loss: 5.5766 - accuracy: 0.0122\n",
            "Epoch 4/5\n",
            "42/42 [==============================] - 2305s 55s/step - loss: 5.5741 - accuracy: 0.0122\n",
            "Epoch 5/5\n",
            "42/42 [==============================] - 2515s 60s/step - loss: 5.5730 - accuracy: 0.0122\n",
            "11/11 [==============================] - 202s 18s/step - loss: 5.5753 - accuracy: 0.0091\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[5.575272083282471, 0.00909090880304575]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SQ2UVrvd7hp6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.save_weights('./model_PredictDeveloper.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VoLcVSVZ9Kb1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.load_weights('./model_PredictDeveloper.h5')  \n",
        "predicts = model.predict(x_test, batch_size=32)  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PrWghR_B8Dl4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_test = np.asarray(y_enc[train_size:])\n",
        "y_test = decode(le, y_test)\n",
        "y_preds = decode(le, predicts)\n",
        "\n",
        "from sklearn import metrics\n",
        "\n",
        "metrics.confusion_matrix(y_test, y_preds)\n",
        "\n",
        "print(metrics.classification_report(y_test, y_preds))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O4B1IHRYBwFE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 578
        },
        "outputId": "76695bb4-2922-47df-e5cb-887c881846d8"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 127,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_71\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_65 (InputLayer)           [(None, 1000)]       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_66 (InputLayer)           [(None, 1000)]       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Equal_32 (TensorFlo [(None, 1000)]       0           input_65[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "embedding_118 (Embedding)       (None, 1000, 512)    3192320     input_66[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Cast_32 (TensorFlow [(None, 1000)]       0           tf_op_layer_Equal_32[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "dense_1525 (Dense)              (None, 1000, 256)    131328      embedding_118[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_strided_slice_39 (T [(None, 1, 1, 1000)] 0           tf_op_layer_Cast_32[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "lstm_6 (LSTM)                   (None, 128)          197120      dense_1525[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "transformer_36 (Transformer)    (None, 268)          22569484    input_65[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_1526 (Dense)              (None, 268)          34572       lstm_6[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_23 (Concatenate)    (None, 536)          0           transformer_36[0][0]             \n",
            "                                                                 dense_1526[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dense_1528 (Dense)              (None, 268)          143916      concatenate_23[0][0]             \n",
            "==================================================================================================\n",
            "Total params: 26,268,740\n",
            "Trainable params: 26,268,740\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}