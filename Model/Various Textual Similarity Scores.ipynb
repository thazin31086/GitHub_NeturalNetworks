{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled17.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "-HBkNm4snNbJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "4fd4438d-f395-4254-bad8-4d589ead2fda"
      },
      "source": [
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "\n",
        "url = \"https://tfhub.dev/google/universal-sentence-encoder-large/3\"\n",
        "embed = hub.Module(url)\n",
        "\n",
        "sentences_test = [\"Don't remove redo option from the page, I think it is useful\",\"Don't remove redo option from the page, I also think it is useful\",\"I don't think it is useful to have redo option, reomove from the page\"]\n",
        "with tf.Session() as session:\n",
        "\n",
        "  session.run([tf.global_variables_initializer(), tf.tables_initializer()])\n",
        "  sentences_embed = session.run(embed(sentences_test))\n",
        "\n",
        "# For evaluation we use exactly normalized rather than\n",
        "# approximately normalized.\n",
        "sts_encode1 = tf.nn.l2_normalize(sentences_embed[0])\n",
        "sts_encode2 = tf.nn.l2_normalize(sentences_embed[1])\n",
        "cosine_similarities = tf.reduce_sum(tf.multiply(sts_encode1, sts_encode2))\n",
        "clip_cosine_similarities = tf.clip_by_value(cosine_similarities, -1.0, 1.0)\n",
        "sim_scores = 1.0 - tf.acos(clip_cosine_similarities)\n",
        "with tf.Session() as sess:    \n",
        "  print(\"Sentence 1 and 2 Score :\")\n",
        "  print(sim_scores.eval())  \n",
        "\n",
        "#sentence 1 and 3\n",
        "sts_encode1 = tf.nn.l2_normalize(sentences_embed[0])\n",
        "sts_encode2 = tf.nn.l2_normalize(sentences_embed[2])\n",
        "cosine_similarities = tf.reduce_sum(tf.multiply(sts_encode1, sts_encode2))\n",
        "clip_cosine_similarities = tf.clip_by_value(cosine_similarities, -1.0, 1.0)\n",
        "sim_scores = 1.0 - tf.acos(clip_cosine_similarities)\n",
        "with tf.Session() as sess:  \n",
        "  print(\"Sentence 1 and 3 Score :\")\n",
        "  print(sim_scores.eval()) "
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Sentence 1 and 2 Score :\n",
            "0.89120203\n",
            "Sentence 1 and 3 Score :\n",
            "0.5125967\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PFVwEb4pB4Hh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "bbee7ecf-d39a-4237-ee3e-44ad0dbf909a"
      },
      "source": [
        ""
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.89120203\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OQwT4FViDgCw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d458effd-f924-403a-fc7b-ba3c2fee3e66"
      },
      "source": [
        ""
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.5125967\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f_bheMNs6sRV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "3fe30e8a-a152-4a42-aafc-8a280cb5f4e1"
      },
      "source": [
        "print(np.inner(sentences_embed[0], sentences_embed[1]))\n",
        "print(np.inner(sentences_embed[1], sentences_embed[2]))\n",
        "print(np.inner(sentences_embed[0], sentences_embed[2]))"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.99408734\n",
            "0.88090116\n",
            "0.8835519\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0YEVFni32kC-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "de336258-2300-4378-fd65-1b7fe4d579e0"
      },
      "source": [
        "from scipy.spatial import distance\n",
        "print(distance.euclidean(sentences_embed[0], sentences_embed[1]))\n",
        "print(distance.euclidean(sentences_embed[1], sentences_embed[2]))\n",
        "print(distance.euclidean(sentences_embed[0], sentences_embed[2]))"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.10874460637569427\n",
            "0.488055020570755\n",
            "0.48259320855140686\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "49kgNvZ_8hl8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        },
        "outputId": "d1f23c83-c283-4807-fbf7-de59200b14c6"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# list of text documents\n",
        "sentences_test_SVM = [\"Don't remove redo option from the page, I think it is useful\",\"Don't remove redo option from the page, I also think it is useful\",\"I don't think it is useful to have redo option, reomove from the page\"]\n",
        "# create the transform\n",
        "vectorizer = CountVectorizer()\n",
        "# tokenize and build vocab\n",
        "vectorizer.fit(sentences_test)\n",
        "# summarize\n",
        "print(vectorizer.vocabulary_)\n",
        "# encode document\n",
        "vector = vectorizer.transform(sentences_test)\n",
        "# summarize encoded vector\n",
        "print(vector.shape)\n",
        "print(type(vector))\n",
        "print(vector.toarray())\n",
        "\n",
        "print(\"Sentence 1 and 2 Score :\")\n",
        "print(cosine_similarity(vector[0].toarray(), vector[1].toarray()))\n",
        "print(\"Sentence 1 and 3 Score :\")\n",
        "print(cosine_similarity(vector[0].toarray(), vector[2].toarray()))"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'don': 1, 'remove': 9, 'redo': 8, 'option': 6, 'from': 2, 'the': 11, 'page': 7, 'think': 12, 'it': 5, 'is': 4, 'useful': 14, 'also': 0, 'to': 13, 'have': 3, 'reomove': 10}\n",
            "(3, 15)\n",
            "<class 'scipy.sparse.csr.csr_matrix'>\n",
            "[[0 1 1 0 1 1 1 1 1 1 0 1 1 0 1]\n",
            " [1 1 1 0 1 1 1 1 1 1 0 1 1 0 1]\n",
            " [0 1 1 1 1 1 1 1 1 0 1 1 1 1 1]]\n",
            "Sentence 1 and 2 Score :\n",
            "[[0.95742711]]\n",
            "Sentence 1 and 3 Score :\n",
            "[[0.83624201]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8NJ4VgmVvd4m",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "616d98d0-68c2-41fb-878b-33392ef2dbf5"
      },
      "source": [
        "import spacy\n",
        "\n",
        "nlp = spacy.load('en')\n",
        "\n",
        "#search_doc = nlp(\"Add brace new line code style for switch expressions\")\n",
        "#main_doc = nlp(\"New line setting for C# switch expression is missing\")\n",
        "\n",
        "search_doc = nlp(\"Don''t remove redo option from the page, I think it is useful\",)\n",
        "main_doc = nlp(\"I don't think it is useful to have redo option, reomove from the page\")\n",
        "print(main_doc.similarity(search_doc))"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.8026810906443756\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n"
          ],
          "name": "stderr"
        }
      ]
    }
  ]
}